_target_: tonic.torch.agents.D4PG
model:
  _target_: tonic.torch.models.ActorCriticWithTargets
  actor:
    _target_: tonic.torch.models.Actor
    encoder:
      _target_: tonic.torch.models.ObservationEncoder
    torso:
      _target_: tonic.torch.models.MLP
      sizes: [256, 256]
      activation:
        _target_: torch.nn.ReLU
        _partial_: true
    head:
      _target_: tonic.torch.models.DeterministicPolicyHead
  critic:
    _target_: tonic.torch.models.Critic
    encoder:
      _target_: tonic.torch.models.ObservationActionEncoder
    torso:
      _target_: tonic.torch.models.MLP
      sizes: [256, 256]
      activation:
        _target_: torch.nn.ReLU
        _partial_: true
    head:
      _target_: tonic.torch.models.DistributionalValueHead
      # These value limits are for DeepMind Control Suite with 0.99 discount,
      # calculated by summing the geometric series:
      #   vmax = max_reward * (1 - discount ** max_length) / (1 - discount)
      #        = 1 * (1 - 0.99 ** 1000) / (1 - 0.99)
      #        = ~100
      #   vmin = -vmax
      # According to https://github.com/google-deepmind/acme/issues/57:
      # "Historically, a value of 150 was used to give it a bit of slack."
      # "The instantaneous rewards are bound in (-1, 1)". This is wrong, as
      # the rewards are bound in [0, 1], so negative value is impossible.
      # But we keep these value limits to be consistent with Tonic's benchmarking.
      _args_: [-150., 150., 51]
  observation_normalizer:
    _target_: tonic.torch.normalizers.MeanStd
replay:
  _target_: tonic.replays.Buffer
  return_steps: 5