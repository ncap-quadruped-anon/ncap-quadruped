# @package _global_
# Train architecture with D4PG algorithm.
name: ???

defaults:
  - /infra/base
  - /tonic: train
  - /quadruped/arena@arena: floor
  - /quadruped/task@task: walk_witp
  - _self_

# ====================
robot: ???
model: ???
# ====================

seed: 0
target_vx: 0.5
arena: ???

task:
  robot: ${robot}
  arena: ${arena}
  target_vx: ${target_vx}
  termination_roll: 45
  termination_pitch: 45
  ground_friction:
    _target_: dm_control.composer.variation.distributions.Uniform
    low: [0.75, 0.005, 0.0001]
    high: [1.25, 0.005, 0.0001]
wrappers: []

# Parameters [vmin, vmax, atoms] for `models.DistributionalValueHead`
#   vmax = max_reward * (1 - discount ** max_length) / (1 - discount)
#        = max_reward * (1 - 0.99 ** (15 sec / 0.03 sec per step)) / (1 - 0.99)
#        = max_reward * ~100
value_distribution: [0., 150., 51] # walk_witp

tonic:
  seed: ${seed}
  agent:
    _target_: tonic.torch.agents.D4PG
    model: ${model}
    replay:
      _target_: tonic.replays.Buffer
      return_steps: 5
  environment:
    _target_: tonic.environments.Composer
    env:
      _target_: dm_control.composer.Environment
      task: ${task}
      time_limit: 15
    wrappers: ${wrappers}
    # Already scaling actions in `robot.pos_transform`. Besides, the Tonic scaling function performs a suboptimal
    # 2-point interpolation (min, max) vs. our 3-point interpolation (min, mid, max) that allows for asymmetric
    # joint ranges (where neutral posture is not exactly midway).
    scaled_actions: false
  trainer:
    steps: 1_000_000
    save_steps: 200_000
  parallel: ${sum:${compute.cpu},-1}

compute:
  cpu: 17
  gpu: 0
  mem: 16
  time: ${prod:60,7} # 7h
