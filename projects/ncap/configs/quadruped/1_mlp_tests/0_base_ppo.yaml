# @package _global_
# Train architecture with PPO algorithm.
name: ???

defaults:
  - /infra/base
  - /tonic: train
  - /quadruped/arena@arena: floor
  - /quadruped/task@task: walk_witp
  - _self_

# ====================
robot: ???
model: ???
# ====================

seed: 0
target_vx: 0.5
arena: ???

task:
  robot: ${robot}
  arena: ${arena}
  target_vx: ${target_vx}
  termination_roll: 45
  termination_pitch: 45
  ground_friction:
    _target_: dm_control.composer.variation.distributions.Uniform
    low: [0.75, 0.005, 0.0001]
    high: [1.25, 0.005, 0.0001]
wrappers: []

tonic:
  seed: ${seed}
  agent:
    _target_: tonic.torch.agents.PPO
    model: ${model}
  environment:
    _target_: tonic.environments.Composer
    env:
      _target_: dm_control.composer.Environment
      task: ${task}
      time_limit: 15
    wrappers: ${wrappers}
    # Already scaling actions in `robot.pos_transform`. Besides, the Tonic scaling function performs a suboptimal
    # 2-point interpolation (min, max) vs. our 3-point interpolation (min, mid, max) that allows for asymmetric
    # joint ranges (where neutral posture is not exactly midway).
    scaled_actions: false
  trainer:
    steps: 10_000_000
    save_steps: 500_000
  parallel: ${sum:${compute.cpu},-1}

compute:
  cpu: 17
  gpu: 0
  mem: 16
  time: ${prod:60,24} # 24h
